{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from scipy.stats import skew, norm\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-clinton",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = pd.read_csv('C:\\\\Users\\\\user\\\\Desktop\\\\Jupyter\\\\CUAI\\\\3월_IMC\\\\Adv_IMC_train.csv')\n",
    "dtest = pd.read_csv('C:\\\\Users\\\\user\\\\Desktop\\\\Jupyter\\\\CUAI\\\\3월_IMC\\\\Adv_IMC_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-principal",
   "metadata": {},
   "source": [
    "# Let's take a look into the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-alberta",
   "metadata": {},
   "source": [
    "### TypeName\n",
    "We can see 'TypeName' has the same values in the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtrain['TypeName'].unique().tolist())\n",
    "print(dtest['TypeName'].unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-camping",
   "metadata": {},
   "source": [
    "### Inches\n",
    "We can use the 'Inches' feature by deleting the NaN values and changing the rest into floats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-baseball",
   "metadata": {},
   "source": [
    "### ScreenResolution\n",
    "In the 'ScreenResolution' column we can see that every value has a (number)x(number) at the end so he can take this value and store it in another column. Also we can find that there are a few words that repeatedly shows up in the values.\n",
    "\n",
    "For example) 'Full HD', 'Touchscreen', 'Quad HD+' etc.\n",
    "I believe that these words have meaning when it comes to predicting the prices of the laptops so we will process them into individual columns as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain['ScreenResolution'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-soldier",
   "metadata": {},
   "source": [
    "### Cpu\n",
    "We can see that the Cpu feature contains Intel and AMD Cpu's. Which we will have to convert into individual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain['Cpu'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-absolute",
   "metadata": {},
   "source": [
    "### Ram\n",
    "Ram can easily be processed by just deleting the 'GB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain['Ram'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-ending",
   "metadata": {},
   "source": [
    "### Memory\n",
    "We can do the same stuff we did with the ScreenResolution column to the Memory column but just a little differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain['Memory'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-dance",
   "metadata": {},
   "source": [
    "### Gpu\n",
    "basically the same thing as the Cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain['Gpu'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-heaven",
   "metadata": {},
   "source": [
    "### OpSys\n",
    "We can see that the values for the train data and the test data are different. So I got rid of the Android value because I considered it as noise(I also didn't want to waste a column just for the 'android' value) and dropped it. Then I put the Windows OS'/Mac OS' into 1 column with different integer values(ex: Windows7 = 1, Windows10 = 2 etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain.OpSys.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest.OpSys.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-symposium",
   "metadata": {},
   "source": [
    "### Weight\n",
    "The Weight column can also be used by just deleting the 'Kg' and then changing it into a float just like what we did with the 'Ram'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain.Weight.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-heath",
   "metadata": {},
   "source": [
    "# Drop noise\n",
    "row 889 - has Samsung Cortex for Cpu and ARM Mali for Gpu(which is not in the test set and has nothing in common with the other index's)\n",
    "\n",
    "row 16, 287, 314, 920, 956 = Intel Core M without further info / 889 has Samsung Cortex for Cpu and ARM Mali for Gpu(which is not in the test set and has nothing in common with the other index's)\n",
    "\n",
    "row 219 contains 'Intel Iris Pro Graphics' which doesn't contain any information about its serial number nor generation.\n",
    "\n",
    "row 268 & 712 contains 'Android' in OpSys which doesn't exist in the test set and there are only 2 rows containing 'Android' so I assumed the two rows as noise and dropped them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(dtrain.values == 'Android')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.where(dtrain.values == 'Intel Iris Pro Graphics')\n",
    "#np.where(dtrain.values == 'Android')\n",
    "#dtrain.Gpu[219]\n",
    "#dtrain.OpSys[268]\n",
    "#dtrain.OpSys[712]\n",
    "dtrain.drop(dtrain.index[[16, 219, 268, 287, 314, 712, 889, 920, 956]], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-daniel",
   "metadata": {},
   "source": [
    "# Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtrain.duplicated().sum()\n",
    "#dtrain.loc[dtrain.duplicated(keep='last'),:] # keep = first / last / False\n",
    "dtrain.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-camping",
   "metadata": {},
   "source": [
    "# Drop NaN in Inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = dtrain.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-amount",
   "metadata": {},
   "source": [
    "# Reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-tongue",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain.reset_index(drop=True, inplace=True) # reset the index to 0~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-repeat",
   "metadata": {},
   "source": [
    "# Skewed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skewness and kurtosis\n",
    "print(\"Skewness: %f\" % dtrain['price'].skew())\n",
    "print(\"Kurtosis: %f\" % dtrain['price'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#standardizing data\n",
    "\n",
    "price_scaled = StandardScaler().fit_transform(dtrain['price'][:,np.newaxis]);\n",
    "low_range = price_scaled[price_scaled[:,0].argsort()][:10]\n",
    "high_range= price_scaled[price_scaled[:,0].argsort()][-10:]\n",
    "\n",
    "print('outer range (low) of the distribution:')\n",
    "print(low_range)\n",
    "print('\\nouter range (high) of the distribution:')\n",
    "print(high_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain['price'] = np.log1p(dtrain['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-cathedral",
   "metadata": {},
   "source": [
    "# Combine train and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = dtrain['price'].reset_index(drop=True)\n",
    "train = dtrain.drop(['price'], axis=1)\n",
    "test = dtest\n",
    "features = pd.concat([train, test]).reset_index(drop=True)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-heritage",
   "metadata": {},
   "source": [
    "# Preprocessing 'Cpu' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CPU_transform(data):\n",
    "    \n",
    "    # Extract GHz from Cpu\n",
    "    data['Cpu'] = data[\"Cpu\"].str.replace(\" \", \"+\")\n",
    "    data['GHz'] = data[\"Cpu\"].str.replace(r\".+[+]\", \"\").str.replace(r\"GHz\", \"\").astype(float)\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    # Intel Xeon E3\n",
    "    data['Cpu_Xeon'] = data[\"Cpu\"].str.replace(r\"Intel[+]Xeon[+]E3[-]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D\", \"\")\n",
    "    data['Cpu_Xeon'] = data['Cpu_Xeon'].replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "    \n",
    "    # Intel Core M - 가격 / 출시 날짜를 고려하여 인코딩\n",
    "    data['Cpu_CoreM'] = data[\"Cpu\"].str.replace(r\"Intel[+]Core[+]M[+]\", \"\").str.replace(r\"M\\d[-]\", \"\").str.replace(r\"6Y30\", \"1\").str.replace(r\"6Y54\", \"1\").str.replace(r\"6Y75\", \"2\").str.replace(r\"7Y30\", \"3\")\n",
    "    data['Cpu_CoreM'] = data[\"Cpu_CoreM\"].str.replace(r\"[+].+\", \"\").str.replace(r\"\\D\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "    \n",
    "    # Intel Pentium Dual / Quad Core\n",
    "    data['Cpu_Pentium2'] = data[\"Cpu\"].str.replace(r\"Intel[+]Pentium[+]Dual[+]Core[+]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "    data['Cpu_Pentium4'] = data[\"Cpu\"].str.replace(r\"Intel[+]Pentium[+]Quad[+]Core[+]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "\n",
    "    # Intel Atom\n",
    "    data['Cpu_Atom'] = data[\"Cpu\"].str.replace(r\"Intel[+]Atom[+]\\D\\d[-]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "\n",
    "    # Intel Celeron Dual / Quad Core\n",
    "    data['Cpu_Celeron2'] = data[\"Cpu\"].str.replace(r\"Intel[+]Celeron[+]Dual[+]Core[+]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "    data['Cpu_Celeron4'] = data[\"Cpu\"].str.replace(r\"Intel[+]Celeron[+]Quad[+]Core[+]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "\n",
    "    # Intel Core i-series\n",
    "    data['Cpu_i3'] = data[\"Cpu\"].str.replace(r\"Intel[+]Core[+]i3[+]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "    data['Cpu_i5'] = data[\"Cpu\"].str.replace(r\"Intel[+]Core[+]i5[+]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "    data['Cpu_i7'] = data[\"Cpu\"].str.replace(r\"Intel[+]Core[+]i7[+]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "\n",
    "    # AMD_A Series\n",
    "    data['Cpu_AMD_A'] = data[\"Cpu\"].str.replace(r\"AMD[+]A\", \"\").str.replace(r\"[-].+[+].+[-]\", \"\").str.replace(r\"[-]\\D+[+]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "\n",
    "    # AMD Ryzen\n",
    "    data['Cpu_AMD_Ryzen'] = data[\"Cpu\"].str.replace(r\"AMD[+]Ryzen[+]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "\n",
    "    # AMD FX\n",
    "    data['Cpu_AMD_FX'] = data[\"Cpu\"].str.replace(r\"AMD[+]FX[+]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "\n",
    "    # AMD E-Series\n",
    "    data['Cpu_AMD_E'] = data[\"Cpu\"].str.replace(r\"AMD[+]E[-]\\D+\", \"\").str.replace(r\".+[-]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = CPU_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-university",
   "metadata": {},
   "source": [
    "# Preprocessing 'Gpu' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPU_transform(data):\n",
    "    \n",
    "    # AMD R17M-M1-70 is the same as AMD Radeon R7 M530\n",
    "    data['Gpu'] = data[\"Gpu\"].str.replace(\"AMD R17M-M1-70\", \"AMD Radeon R7 M530\")\n",
    "    data['Gpu'] = data[\"Gpu\"].str.replace(\" \", \"+\")\n",
    "    \n",
    "    # Intel HD Graphics\n",
    "    # 참고문헌 - https://en.wikipedia.org/wiki/List_of_Intel_graphics_processing_units\n",
    "    for a in data[(data['Gpu'] == 'Intel+HD+Graphics') & ((data['Cpu'] == 'Intel+Atom+x5-Z8350+1.44GHz') | (data['Cpu'] == 'Intel+Atom+X5-Z8350+1.44GHz') | (data['Cpu'] == 'Intel+Celeron+Dual+Core+N3060+1.6GHz'))].index:\n",
    "        data['Gpu'][a] = data['Gpu'][a].replace('Intel+HD+Graphics', 'Intel+HD+Graphics+400')\n",
    "    for b in data[(data['Gpu'] == 'Intel+HD+Graphics') & (data['Cpu'] == 'Intel+Pentium+Quad+Core+N3710+1.6GHz')].index:\n",
    "        data['Gpu'][b] = data['Gpu'][b].replace('Intel+HD+Graphics', 'Intel+HD+Graphics+405')\n",
    "    for c in data[(data['Gpu'] == 'Intel+HD+Graphics') & ((data['Cpu'] == 'Intel+Core+i5+7200U+2.5GHz') | (data['Cpu'] == 'Intel+Core+i7+7600U+2.8GHz'))].index:\n",
    "        data['Gpu'][c] = data['Gpu'][c].replace('Intel+HD+Graphics', 'Intel+HD+Graphics+620')\n",
    "    # 남아 있는 Intel HD Graphics들은 전부 8세대이므로 묶어줍니다.\n",
    "    data['Gpu_HDG_default'] = data[\"Gpu\"].str.replace(r\"Intel[+]HD[+]Graphics\", \"\").replace(r'^\\s*$', 1, regex=True).str.replace(r\"[+].+\", \"\").replace(np.nan, 1, regex=True).str.replace(r\"\\D\", \"\").replace(np.nan, 1, regex=True).replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "    data['Gpu_Intel_HD'] = data[\"Gpu\"].str.replace(r\"Intel[+]HD[+]Graphics[+]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "    \n",
    "    # Intel UHD Graphics\n",
    "    data['Gpu_Intel_UHD'] = data[\"Gpu\"].str.replace(r\"Intel[+]UHD[+]\\D+[+]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "    \n",
    "    # Intel Iris (default)/Plus\n",
    "    data['Gpu_Intel_Iris'] = data[\"Gpu\"].str.replace(r\"Intel[+]Iris[+]Plus[+]\\D+\", \"\").str.replace(r\"Intel[+]Iris[+]\\D+\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "    \n",
    "    # Nvidia Geforce GTX_Ti\n",
    "    data['Gpu_Nvidia_GTX_Ti'] = data[\"Gpu\"].str.replace(r\"Nvidia[+]GeForce[+]GTX.+Ti\", \"1\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "    \n",
    "    # Nvidia GeForce GTX_M\n",
    "    # Ti들을 미리 삭제\n",
    "    data['Gpu_Nvidia_GTX_M'] = data[\"Gpu\"].str.replace(r\"Nvidia[+]GeForce[+]GTX.+Ti\", \"0\") # Ti들을 미리 삭제\n",
    "    data['Gpu_Nvidia_GTX_M'] = data[\"Gpu_Nvidia_GTX_M\"].str.replace(r\"Nvidia[+]GeForce[+]GTX.+MX\", \"\")\n",
    "    data['Gpu_Nvidia_GTX_M'] = data[\"Gpu_Nvidia_GTX_M\"].str.replace(r\"Nvidia[+]GeForce[+]GTX[+]\", \"\")\n",
    "\n",
    "    data['Gpu_Nvidia_GTX_M'] = data[\"Gpu_Nvidia_GTX_M\"].str.replace(r\"[+].+\", \"\")\n",
    "    data['Gpu_Nvidia_GTX_M'] = data[\"Gpu_Nvidia_GTX_M\"].str.replace(r\"[^\\d+M]\", '')\n",
    "    data['Gpu_Nvidia_GTX_M1'] = data['Gpu_Nvidia_GTX_M'].str.split('M')\n",
    "\n",
    "    for i in range(len(data['Gpu_Nvidia_GTX_M'])):\n",
    "        if len(data['Gpu_Nvidia_GTX_M'].str.split('M')[i]) == 2:\n",
    "            data['Gpu_Nvidia_GTX_M1'][i] = data['Gpu_Nvidia_GTX_M'].str.split('M')[i][0]\n",
    "\n",
    "    data['Gpu_Nvidia_GTX_M'] = data['Gpu_Nvidia_GTX_M1'].str.replace(r\"[.+]\", '')\n",
    "    data['Gpu_Nvidia_GTX_M'] = data['Gpu_Nvidia_GTX_M'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    data['Gpu_Nvidia_GTX_M'] = data['Gpu_Nvidia_GTX_M'].fillna(0)\n",
    "    data['Gpu_Nvidia_GTX_M'] = data['Gpu_Nvidia_GTX_M'].astype(int)\n",
    "    \n",
    "    # Nvidia GeForce GTX_MX\n",
    "    # Ti들을 미리 삭제\n",
    "    data['Gpu_Nvidia_GTX_MX'] = data[\"Gpu\"].str.replace(r\"Nvidia[+]GeForce[+]GTX.+Ti\", \"0\") # Ti들을 미리 삭제\n",
    "    data['Gpu_Nvidia_GTX_MX'] = data[\"Gpu_Nvidia_GTX_MX\"].str.replace(r\"Nvidia[+]GeForce[+]GTX[+]\", \"\")\n",
    "\n",
    "    data['Gpu_Nvidia_GTX_MX'] = data[\"Gpu_Nvidia_GTX_MX\"].str.replace(r\"[+].+\", \"\")\n",
    "    data['Gpu_Nvidia_GTX_MX'] = data[\"Gpu_Nvidia_GTX_MX\"].str.replace(r\"[^\\d+MX]\", '')\n",
    "    data['Gpu_Nvidia_GTX_MX1'] = data['Gpu_Nvidia_GTX_MX'].str.split('MX')\n",
    "\n",
    "    for i in range(len(data['Gpu_Nvidia_GTX_MX'])):\n",
    "        if len(data['Gpu_Nvidia_GTX_MX'].str.split('MX')[i]) == 2:\n",
    "            data['Gpu_Nvidia_GTX_MX1'][i] = data['Gpu_Nvidia_GTX_MX'].str.split('M')[i][0]\n",
    "\n",
    "    data['Gpu_Nvidia_GTX_MX'] = data['Gpu_Nvidia_GTX_MX1'].str.replace(r\"[.+]\", '')\n",
    "    data['Gpu_Nvidia_GTX_MX'] = data['Gpu_Nvidia_GTX_MX'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    data['Gpu_Nvidia_GTX_MX'] = data['Gpu_Nvidia_GTX_MX'].fillna(0)\n",
    "    data['Gpu_Nvidia_GTX_MX'] = data['Gpu_Nvidia_GTX_MX'].astype(int)\n",
    "    \n",
    "    # Nvidia GeForce GTX\n",
    "    # Ti들을 미리 삭제\n",
    "    data['Gpu_Nvidia_GTX'] = data[\"Gpu\"].str.replace(r\"Nvidia[+]GeForce[+]GTX.+Ti\", \"0\").str.replace(r\"Nvidia[+]GeForce[+]GTX.+MX?\", \"\").str.replace(r\"Nvidia[+]GeForce[+]GTX[+]?\", \"\").str.replace(r\"<.+>\", \"\")\n",
    "    # Nvidia GTX 980 SLI drop 하긴 아까운 데이터라 그냥 이렇게 처리\n",
    "    data['Gpu_Nvidia_GTX'] = data[\"Gpu_Nvidia_GTX\"].str.replace(r\"Nvidia[+]GTX[+]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D+\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "    \n",
    "    # Nvidia GeForce GT\n",
    "    #GTX 들을 미리 삭제\n",
    "    data['Gpu_Nvidia_GT'] = data[\"Gpu\"].str.replace(r\"Nvidia[+]GeForce[+]GTX.+\", \"\").str.replace(r\"Nvidia[+]GeForce[+]GT[+]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D+\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "    \n",
    "    # Nvidia Quadro\n",
    "    data['Gpu_Nvidia_Q'] = data[\"Gpu\"].str.replace(r\"Nvidia[+]Quadro[+]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D+\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "    \n",
    "    # Nvidia GeForce(M, MX, default)구분없이\n",
    "    data['Gpu_Nvidia_MX'] = data[\"Gpu\"].str.replace(r\"Nvidia[+]GeForce[+]\", \"\").str.replace(r\"GT.+\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D+\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "    \n",
    "    # AMD Radeon R_Series\n",
    "    data['Gpu_AMD_R'] = data[\"Gpu\"].str.replace(r\"AMD[+]Radeon[+]R\", \"\").str.replace(r\"AMD[+]FirePro.+\", \"\").str.replace(r\"Intel.+\", \"\").str.replace(r\"Nvidia.+\", \"\").str.replace(r\"[+]M\", \".\").str.replace(r\"X\", \"10\").str.replace(r\"[+]\", \".\").str.replace(r\"AMD.Radeon.+\", \"\").str.replace(r\"Graphics\", \"\").str.replace(r\"AMD.R\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(float)\n",
    "    \n",
    "    # AMD FirePro\n",
    "    data['Gpu_AMD_FP'] = data[\"Gpu\"].str.replace(r\"AMD[+]FirePro[+]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D+\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "    \n",
    "    # AMD Pro\n",
    "    data['Gpu_AMD_Pro'] = data[\"Gpu\"].str.replace(r\"AMD[+]Radeon[+]Pro[+]\", \"\").str.replace(r\"[+].+\", \"\").str.replace(r\"\\D+\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "    \n",
    "    # AMD Radeon 이후에 숫자\n",
    "    data['Gpu_AMD_NUM'] = data[\"Gpu\"].str.replace(r\"AMD[+]Radeon[+]R.+\", \"\").str.replace(r\"AMD[+]Radeon[+]Pro.+\", \"\").str.replace(r\"AMD[+]Radeon[+]\", \"\").str.replace(r\"AMD[+]FirePro.+\", \"\").str.replace(r\"Intel.+\", \"\").str.replace(r\"Nvidia.+\", \"\").str.replace(r\"\\D+\", \"\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).astype(int)\n",
    "\n",
    "    data = data.drop(['Gpu_Nvidia_GTX_M1','Gpu_Nvidia_GTX_MX1'],axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = GPU_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-nothing",
   "metadata": {},
   "source": [
    "# ScreenResolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SR_transform(data):\n",
    "    \n",
    "    data['SR_4K'] = data['ScreenResolution'].str.findall('.*4K Ultra HD.*')\n",
    "    data['SR_4K'] = data['SR_4K'].explode('SR_4K').str.replace(r'.+', '1').replace(np.nan, 0, regex=True).astype(int)\n",
    "\n",
    "    data['SR_Touch'] = data['ScreenResolution'].str.findall('.*Touchscreen.*')\n",
    "    data['SR_Touch'] = data['SR_Touch'].explode('SR_Touch').str.replace(r'.+', '1').replace(np.nan, 0, regex=True).astype(int)\n",
    "\n",
    "    data['SR_QuadHD'] = data['ScreenResolution'].str.findall('.*Quad HD.*')\n",
    "    data['SR_QuadHD'] = data['SR_QuadHD'].explode('SR_QuadHD').str.replace(r'.+', '1').replace(np.nan, 0, regex=True).astype(int)\n",
    "\n",
    "    data['SR_Retina'] = data['ScreenResolution'].str.findall('.*Retina Display.*')\n",
    "    data['SR_Retina'] = data['SR_Retina'].explode('SR_Retina').str.replace(r'.+', '1').replace(np.nan, 0, regex=True).astype(int)\n",
    "\n",
    "    data['SR_FullHD'] = data['ScreenResolution'].str.findall('.*Full HD.*')\n",
    "    data['SR_FullHD'] = data['SR_FullHD'].explode('SR_FullHD').str.replace(r'.+', '1').replace(np.nan, 0, regex=True).astype(int)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = SR_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Memory_transform(data):\n",
    "    \n",
    "    data['Memory'] = data['Memory'].astype(str).replace('\\.0', '', regex=True) \n",
    "    data[\"Memory\"] = data[\"Memory\"].str.replace('GB', '')\n",
    "    data[\"Memory\"] = data[\"Memory\"].str.replace('TB', '000')\n",
    "    SSD_HDD_FS_Hy = data[\"Memory\"].str.split(\"+\", n = 1, expand = True)\n",
    "    data[\"first\"]= SSD_HDD_FS_Hy[0]\n",
    "    data[\"first\"]= data[\"first\"].str.strip()\n",
    "    data[\"second\"]= SSD_HDD_FS_Hy[1]\n",
    "    data[\"HDD1\"] = data[\"first\"].apply(lambda data: 1 if \"HDD\" in data else 0)\n",
    "    data[\"SSD1\"] = data[\"first\"].apply(lambda data: 1 if \"SSD\" in data else 0)\n",
    "    data[\"Hybrid1\"] = data[\"first\"].apply(lambda data: 1 if \"Hybrid\" in data else 0)\n",
    "    data[\"Flash_Storage1\"] = data[\"first\"].apply(lambda data: 1 if \"Flash Storage\" in data else 0)\n",
    "    data['first'] = data['first'].str.replace(r'\\D', '')\n",
    "    data[\"second\"].fillna(\"0\", inplace = True)\n",
    "    data[\"HDD2\"] = data[\"second\"].apply(lambda data: 1 if \"HDD\" in data else 0)\n",
    "    data[\"SSD2\"] = data[\"second\"].apply(lambda data: 1 if \"SSD\" in data else 0)\n",
    "    data[\"Hybrid2\"] = data[\"second\"].apply(lambda data: 1 if \"Hybrid\" in data else 0)\n",
    "    data[\"Flash_Storage2\"] = data[\"second\"].apply(lambda data: 1 if \"Flash Storage\" in data else 0)\n",
    "    data['second'] = data['second'].str.replace(r'\\D', '')\n",
    "    data[\"first\"] = data[\"first\"].astype(int)\n",
    "    data[\"second\"] = data[\"second\"].astype(int)\n",
    "    data['HDD'] = data[\"first\"]*data['HDD1'] + data[\"second\"]*data['HDD2']\n",
    "    data['SSD'] = data[\"first\"]*data['SSD1'] + data[\"second\"]*data['SSD2']\n",
    "    data['Hybrid'] = data[\"first\"]*data['Hybrid1'] + data[\"second\"]*data['Hybrid2']\n",
    "    data['Flash_Storage'] = data[\"first\"]*data['Flash_Storage1'] + data[\"second\"]*data['Flash_Storage2']\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Memory_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SR_XY_transform(data):\n",
    "    \n",
    "    SR = data[\"ScreenResolution\"].str.split(\"x\", n = 1, expand = True)\n",
    "    data[\"xres\"]= SR[0]\n",
    "    data[\"yres\"]= SR[1]\n",
    "    data[\"xres\"] = data['xres'].str.replace(r'\\D+.\\d?\\D+', '').astype(int)\n",
    "    data[\"yres\"] = data[\"yres\"].astype(int)\n",
    "    data[\"ScreenResolution\"]=(data[\"xres\"]*data[\"yres\"]).astype(int)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = SR_XY_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ram_transform(data):\n",
    "    \n",
    "    data[\"Ram\"] = data[\"Ram\"].str.replace('GB', '') ## remove 'GB'\n",
    "    data[\"Ram\"] = data[\"Ram\"].astype(int)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Ram_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weight_transform(data):\n",
    "    \n",
    "    data[\"Weight\"] = data[\"Weight\"].str.replace('kg', '') ## remove 'kg'\n",
    "    data[\"Weight\"] = data[\"Weight\"].astype(float)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-measure",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Weight_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Drop_transform(data):\n",
    "    \n",
    "    data = data.drop(['Cpu', 'Gpu', 'Memory','first',\n",
    "                'second','HDD1','SSD1','Hybrid1','Flash_Storage1',\n",
    "                'HDD2','SSD2','Hybrid2','Flash_Storage2'],axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Drop_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-adolescent",
   "metadata": {},
   "source": [
    "# Skewed data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewcolumn = ['Inches','ScreenResolution','Ram','Weight','GHz','HDD','SSD','Hybrid','Flash_Storage','xres','yres']\n",
    "for i in skewcolumn:\n",
    "    print('{} : {}'.format(i, features[i].skew()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['ScreenResolution'] = np.log1p(features['ScreenResolution'])\n",
    "features['Weight'] = np.log1p(features['Weight'])\n",
    "features['xres'] = np.log1p(features['xres'])\n",
    "features['yres'] = np.log1p(features['yres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-semester",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-technical",
   "metadata": {},
   "source": [
    "# One-Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_TypeName(data):\n",
    "    TN = pd.get_dummies(data['TypeName'])\n",
    "    return TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = one_hot_TypeName(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_OpSys(data):\n",
    "    OS = pd.get_dummies(data['OpSys'])\n",
    "    \n",
    "    OS['Windows'] = OS['Windows 7'] + OS['Windows 10 S'] + OS['Windows 10']\n",
    "    OS = OS.drop(['Windows 7', 'Windows 10 S', 'Windows 10'], axis=1)\n",
    "    OS['MacOS'] = OS['Mac OS X']*1 + OS['macOS']*2 # macOS가 더 좋은 OS\n",
    "    OS = OS.drop(['Mac OS X', 'macOS'], axis=1)\n",
    "    return OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "OS = one_hot_OpSys(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.Company.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_Company(data):\n",
    "    Company = pd.get_dummies(data['Company'])\n",
    "    return Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-corporation",
   "metadata": {},
   "outputs": [],
   "source": [
    "CP = one_hot_Company(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_Product(data):\n",
    "    Product = pd.get_dummies(data['Product'])\n",
    "    return Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pr = one_hot_Product(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_dummy():\n",
    "    for i in TN.columns:\n",
    "        features[i] = TN[i]\n",
    "    for i in OS.columns:\n",
    "        features[i] = OS[i]\n",
    "    for i in CP.columns:\n",
    "        features[i] = CP[i]\n",
    "    for i in Pr.columns:\n",
    "        features[i] = Pr[i]\n",
    "\n",
    "input_dummy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = features.iloc[:len(train_labels), :]\n",
    "test = features.iloc[len(train_labels):, :]\n",
    "train.shape, train_labels.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-republic",
   "metadata": {},
   "source": [
    "# BayesianOptimizer\n",
    "With the BayesianOptimizer we will kind the ideal values for the parameters in each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, XGBRegressor\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-prayer",
   "metadata": {},
   "source": [
    "# Models\n",
    "Find what each parameters mean in each model by looking into the link below every model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-theme",
   "metadata": {},
   "source": [
    "## LightGBM\n",
    "LightGBM - https://neptune.ai/blog/lightgbm-parameters-guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-thursday",
   "metadata": {},
   "source": [
    "## XGB Regressor\n",
    "https://xgboost.readthedocs.io/en/latest/parameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-holmes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 탐색 대상 함수 (XGBRegressor)\n",
    "def XGB_cv(max_depth, learning_rate, n_estimators, gamma\n",
    "             ,min_child_weight, subsample\n",
    "             ,colsample_bytree, reg_alpha, reg_lambda, objective='reg:linear', silent=True, nthread=-1):\n",
    "\n",
    "    # 모델 정의\n",
    "    model = XGBRegressor(max_depth=int(max_depth),\n",
    "                           learning_rate=learning_rate,\n",
    "                           n_estimators=int(n_estimators),\n",
    "                           gamma=gamma,\n",
    "                           min_child_weight=min_child_weight,\n",
    "                           subsample=subsample,\n",
    "                           colsample_bytree=colsample_bytree,\n",
    "                           reg_alpha=reg_alpha,\n",
    "                           reg_lambda = reg_lambda,\n",
    "                           objective=objective,\n",
    "                           nthread=nthread\n",
    "                           )\n",
    "\n",
    "    # metric 계산\n",
    "    rmse = np.sqrt(-cross_val_score(model, train, train_labels, scoring=\"neg_mean_squared_error\", cv=5))\n",
    "\n",
    "    \n",
    "    # 오차 최적화로 사용할 metric 반환\n",
    "    return -rmse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험해보고자하는 hyperparameter 집합\n",
    "pbounds = {'max_depth': (3, 8),\n",
    "            'learning_rate': (0.001, 0.1),\n",
    "            'n_estimators': (1000, 10000),\n",
    "            'gamma': (0, 1),\n",
    "            'min_child_weight': (0, 3),\n",
    "            'subsample': (0.5, 1),\n",
    "            'colsample_bytree' : (0.2, 1),\n",
    "            'reg_alpha' : (0,5),\n",
    "            'reg_lambda' : (0,10),\n",
    "            }\n",
    "\n",
    "bo=BayesianOptimization(f=XGB_cv, pbounds=pbounds, verbose=2, random_state=42)\n",
    "\n",
    "bo.maximize(init_points=10, n_iter=30, acq='ei', xi=0.01)\n",
    "\n",
    "print(bo.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'target': -0.1841129313945096, 'params': {'colsample_bytree': 0.5152651581814524, 'gamma': 0.021071952664826532, 'learning_rate': 0.07603995749131016, 'max_depth': 6.809844347927643, 'min_child_weight': 2.761301161457484, 'n_estimators': 6136.355874258778, 'reg_alpha': 1.3188365258147017, 'reg_lambda': 1.4406194115140336, 'subsample': 0.7777929105084442}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-tension",
   "metadata": {},
   "source": [
    "## Ridge\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# 탐색 대상 함수 (Ridge)\n",
    "def Ridge_cv(alpha):\n",
    "\n",
    "    # 모델 정의\n",
    "    model = make_pipeline(RobustScaler(), Ridge(alpha=alpha))\n",
    "\n",
    "    # metric 계산\n",
    "    rmse = np.sqrt(-cross_val_score(model, train, train_labels, scoring=\"neg_mean_squared_error\", cv=5))\n",
    "\n",
    "    # 오차 최적화로 사용할 metric 반환\n",
    "    return -rmse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험해보고자하는 hyperparameter 집합\n",
    "pbounds = {'alpha': (0.001, 10)}\n",
    "\n",
    "bo=BayesianOptimization(f=Ridge_cv, pbounds=pbounds, verbose=2, random_state=42)\n",
    "bo.maximize(init_points=20, n_iter=60, acq='ei', xi=0.01)\n",
    "print(bo.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'target': -0.2162339615853202, 'params': {'alpha': 0.5817780380698264}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-grammar",
   "metadata": {},
   "source": [
    "## Lasso\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# 탐색 대상 함수 (Lasso)\n",
    "def Lasso_cv(alpha):\n",
    "\n",
    "    # 모델 정의\n",
    "    model = make_pipeline(RobustScaler(), Lasso(alpha=alpha))\n",
    "\n",
    "    # metric 계산\n",
    "    rmse = np.sqrt(-cross_val_score(model, train, train_labels, scoring=\"neg_mean_squared_error\", cv=5))\n",
    "\n",
    "    # 오차 최적화로 사용할 metric 반환\n",
    "    return -rmse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험해보고자하는 hyperparameter 집합\n",
    "\n",
    "pbounds = {'alpha': (1e-15, 1)}\n",
    "\n",
    "bo=BayesianOptimization(f=Lasso_cv, pbounds=pbounds, verbose=2, random_state=42)\n",
    "bo.maximize(init_points=20, n_iter=60, acq='ei', xi=0.01)\n",
    "print(bo.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'target': -0.24154720402835123, 'params': {'alpha': 1.1634755367141103e-05}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-smith",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# 탐색 대상 함수 (ElasticNet)\n",
    "def ElasticNet_cv(alpha):\n",
    "\n",
    "    # 모델 정의\n",
    "    model = make_pipeline(RobustScaler(), ElasticNet(alpha=alpha))\n",
    "\n",
    "    # metric 계산\n",
    "    rmse = np.sqrt(-cross_val_score(model, train, train_labels, scoring=\"neg_mean_squared_error\", cv=5))\n",
    "\n",
    "    # 오차 최적화로 사용할 metric 반환\n",
    "    return -rmse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험해보고자하는 hyperparameter 집합\n",
    "pbounds = {'alpha': (1e-15, 1)}\n",
    "\n",
    "bo=BayesianOptimization(f=ElasticNet_cv, pbounds=pbounds, verbose=2, random_state=42)\n",
    "bo.maximize(init_points=20, n_iter=60, acq='ei', xi=0.01)\n",
    "print(bo.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'target': -0.24719560709406352, 'params': {'alpha': 1.1634755367141103e-05}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-sperm",
   "metadata": {},
   "source": [
    "## SVR\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# 탐색 대상 함수 (Support Vector Regressor)\n",
    "def SVR_cv(C, epsilon, gamma):\n",
    "\n",
    "    # 모델 정의\n",
    "    model = make_pipeline(RobustScaler(), SVR(C=C, epsilon=epsilon, gamma=gamma))\n",
    "\n",
    "    # metric 계산\n",
    "    rmse = np.sqrt(-cross_val_score(model, train, train_labels, scoring=\"neg_mean_squared_error\", cv=5))\n",
    "\n",
    "    # 오차 최적화로 사용할 metric 반환\n",
    "    return -rmse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험해보고자하는 hyperparameter 집합\n",
    "pbounds = {'C': (0.1, 100),\n",
    "          'epsilon': (1e-8, 0.1),\n",
    "          'gamma': (1e-8, 0.1)}\n",
    "\n",
    "bo=BayesianOptimization(f=SVR_cv, pbounds=pbounds, verbose=2, random_state=42)\n",
    "bo.maximize(init_points=20, n_iter=60, acq='ei', xi=0.01)\n",
    "print(bo.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'target': -0.26173172662582117, 'params': {'C': 51.44116726098502, 'epsilon': 0.0388359599090226, 'gamma': 0.0006357750751616731}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-surveillance",
   "metadata": {},
   "source": [
    "## GBR\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# 탐색 대상 함수 (Gradient Boosting Regressor)\n",
    "def GBR_cv(n_estimators, max_depth, min_samples_leaf, min_samples_split,learning_rate=0.001, max_features='sqrt', loss='huber'):\n",
    "\n",
    "    # 모델 정의\n",
    "    model = GradientBoostingRegressor(n_estimators=int(n_estimators),\n",
    "                                      learning_rate=learning_rate,\n",
    "                                      max_depth=int(max_depth),\n",
    "                                      max_features=max_features,\n",
    "                                      min_samples_leaf=int(min_samples_leaf),\n",
    "                                      min_samples_split=int(min_samples_split),\n",
    "                                      loss=loss\n",
    "                                      )\n",
    "\n",
    "    # metric 계산\n",
    "    rmse = np.sqrt(-cross_val_score(model, train, train_labels, scoring=\"neg_mean_squared_error\", cv=5))\n",
    "\n",
    "    # 오차 최적화로 사용할 metric 반환\n",
    "    return -rmse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험해보고자하는 hyperparameter 집합\n",
    "\n",
    "pbounds = {'n_estimators': (1000, 10000),\n",
    "           'learning_rate': (0.001, 0.1),\n",
    "           'max_depth': (2, 8),\n",
    "           'min_samples_leaf': (5, 50),\n",
    "           'min_samples_split': (5, 50)\n",
    "           }\n",
    "\n",
    "bo=BayesianOptimization(f=GBR_cv, pbounds=pbounds, verbose=2, random_state=42)\n",
    "bo.maximize(init_points=20, n_iter=60, acq='ei', xi=0.01)\n",
    "print(bo.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'target': -0.1757488282065423, 'params': {'learning_rate': 0.07105448853347894, 'max_depth': 4.2815300710953945, 'min_samples_leaf': 5.429323528975727, 'min_samples_split': 45.77064151175414, 'n_estimators': 4919.741973374068}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-slovak",
   "metadata": {},
   "source": [
    "## Random Forest Regressor\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# 탐색 대상 함수 (Gradient Boosting Regressor)\n",
    "def RF_cv(n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features=None, oob_score='True'):\n",
    "\n",
    "    # 모델 정의\n",
    "    model = RandomForestRegressor(n_estimators=int(n_estimators),\n",
    "                                      max_depth=int(max_depth),\n",
    "                                      min_samples_split=int(min_samples_split),\n",
    "                                      min_samples_leaf=int(min_samples_leaf),\n",
    "                                      max_features=max_features,\n",
    "                                      oob_score=oob_score\n",
    "                                      )\n",
    "\n",
    "    # metric 계산\n",
    "    rmse = np.sqrt(-cross_val_score(model, train, train_labels, scoring=\"neg_mean_squared_error\", cv=5))\n",
    "\n",
    "    # 오차 최적화로 사용할 metric 반환\n",
    "    return -rmse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험해보고자하는 hyperparameter 집합\n",
    "pbounds = {'n_estimators': (1000, 6000),\n",
    "           'max_depth': (10, 30),\n",
    "           'min_samples_split': (2, 8),\n",
    "           'min_samples_leaf': (2, 8)\n",
    "           }\n",
    "\n",
    "bo=BayesianOptimization(f=RF_cv, pbounds=pbounds, verbose=2, random_state=42)\n",
    "bo.maximize(init_points=10, n_iter=30, acq='ei', xi=0.01)\n",
    "print(bo.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'target': -0.2080683544796602, 'params': {'max_depth': 22.237057894447588, 'min_samples_leaf': 2.836963163912251, 'min_samples_split': 3.752867891211309, 'n_estimators': 2831.8092164684585}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-glossary",
   "metadata": {},
   "source": [
    "## KNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# 탐색 대상 함수 (K-Neighbors)\n",
    "def KNR_cv(n_neighbors, weights='distance'):\n",
    "\n",
    "    # 모델 정의\n",
    "    model = KNeighborsRegressor(n_neighbors=int(n_neighbors),\n",
    "                                weights=weights\n",
    "                                      )\n",
    "\n",
    "    # metric 계산\n",
    "    rmse = np.sqrt(-cross_val_score(model, train, train_labels, scoring=\"neg_mean_squared_error\", cv=5))\n",
    "\n",
    "    # 오차 최적화로 사용할 metric 반환\n",
    "    return -rmse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험해보고자하는 hyperparameter 집합\n",
    "pbounds = {'n_neighbors': (3, 10)\n",
    "           }\n",
    "\n",
    "bo=BayesianOptimization(f=KNR_cv, pbounds=pbounds, verbose=2, random_state=42)\n",
    "bo.maximize(init_points=20, n_iter=60, acq='ei', xi=0.01)\n",
    "print(bo.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'target': -0.27512379240601337, 'params': {'n_neighbors': 4.092130483097056}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-affect",
   "metadata": {},
   "source": [
    "## Set Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cross validation folds\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define error metrics\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "def mse(y, y_pred):\n",
    "    return np.mean(np.square(y-y_pred))\n",
    "\n",
    "def cv_rmse(model, train=train):\n",
    "    rmse = np.sqrt(-cross_val_score(model, train, train_labels, scoring=\"neg_mean_squared_error\", cv=kf))\n",
    "    return (rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Regressor\n",
    "xgboost = XGBRegressor(learning_rate=0.076,\n",
    "                       n_estimators=6000,\n",
    "                       max_depth=6,\n",
    "                       min_child_weight=2,\n",
    "                       gamma=0.021071952664826532,\n",
    "                       subsample=0.7777929105084442,\n",
    "                       colsample_bytree=0.5152651581814524,\n",
    "                       objective='reg:linear',\n",
    "                       nthread=-1,\n",
    "                       scale_pos_weight=1,\n",
    "                       seed=27,\n",
    "                       reg_alpha=1.3188365258147017,\n",
    "                       reg_lambda=1.4406194115140336,\n",
    "                       verbosity = 0,\n",
    "                       random_state=42)\n",
    "\n",
    "# Ridge Lasso ElasticNet 3인방은 비슷하기 때문에 3 중에서 성능이 제일 좋은 것만 가져갔습니다. - Ridge\n",
    "\n",
    "# Ridge Regressor\n",
    "ridge = make_pipeline(RobustScaler(), Ridge(alpha=0.5817780380698264))\n",
    "\n",
    "# Lasso Regressor\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha=1.1634755367141103e-05))\n",
    "\n",
    "# Elastic Net Regressor\n",
    "elasticnet = make_pipeline(RobustScaler(), ElasticNet(alpha=1.1634755367141103e-05))\n",
    "\n",
    "# Support Vector Regressor\n",
    "svr = make_pipeline(RobustScaler(), SVR(C=51.44116726098502, epsilon=0.0388359599090226, gamma=0.0006357750751616731))\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(n_estimators=5000,\n",
    "                                learning_rate=0.07105448853347894,\n",
    "                                max_depth=4,\n",
    "                                max_features='sqrt',\n",
    "                                min_samples_leaf=5,\n",
    "                                min_samples_split=45,\n",
    "                                loss='huber',\n",
    "                                random_state=42) \n",
    "\n",
    "# Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=3000,\n",
    "                          max_depth=22,\n",
    "                          min_samples_split=3,\n",
    "                          min_samples_leaf=2,\n",
    "                          max_features=None,\n",
    "                          oob_score=True,\n",
    "                          random_state=42)\n",
    "\n",
    "#K-Neighbors Regressor\n",
    "kn = KNeighborsRegressor(n_neighbors=4, weights='distance')\n",
    "\n",
    "# Stack up all the models above, optimized using gbr\n",
    "stack_gen = StackingCVRegressor(regressors=(xgboost, ridge, svr, gbr, rf, kn),\n",
    "                                meta_regressor=gbr,\n",
    "                                use_features_in_secondary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-seventh",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cv_rmse(xgboost)\n",
    "print(\"xgboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['xgb'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cv_rmse(ridge)\n",
    "print(\"ridge: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['ridge'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cv_rmse(lasso)\n",
    "print(\"lasso: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['lasso'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cv_rmse(elasticnet)\n",
    "print(\"elasticnet: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['elasticnet'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cv_rmse(svr)\n",
    "print(\"SVR: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['svr'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cv_rmse(gbr)\n",
    "print(\"gbr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['gbr'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cv_rmse(rf)\n",
    "print(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['rf'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cv_rmse(kn)\n",
    "print(\"kn: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['kn'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-decline",
   "metadata": {},
   "source": [
    "# Fit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-peeing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stack_gen_model = stack_gen.fit(np.array(train), np.array(train_labels))\n",
    "print('complete : stack_gen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_full_data = xgboost.fit(train, train_labels)\n",
    "print('complete : xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model_full_data = ridge.fit(train, train_labels)\n",
    "print('complete : Ridge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_model_full_data = svr.fit(train, train_labels)\n",
    "print('complete : Svr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_model_full_data = gbr.fit(train, train_labels)\n",
    "print('complete : GradientBoosting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-liquid",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_full_data = rf.fit(train, train_labels)\n",
    "print('complete : RandomForest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_model_full_data = kn.fit(train, train_labels)\n",
    "print('complete : KNR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-thanks",
   "metadata": {},
   "source": [
    "# Blend Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to prevent overfitting\n",
    "def blended_predictions(X):\n",
    "    blended = ((0.5 * gbr_model_full_data.predict(X)) + (0.5 * stack_gen_model.predict(np.array(X))))\n",
    "    return blended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final precitions from the blended model\n",
    "blended_score_rmsle = rmsle(train_labels, blended_predictions(train))\n",
    "blended_score_mse = mse(np.expm1(train_labels), np.expm1(blended_predictions(train)))\n",
    "rmse = np.sqrt(blended_score_mse)\n",
    "scores['blended'] = (blended_score_rmsle, 0)\n",
    "print('RMSLE score on train data:')\n",
    "print(blended_score_rmsle)\n",
    "print('MSE score on train data:')\n",
    "print(blended_score_mse)\n",
    "print('RMSE score on train data:')\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how accurate is our model.\n",
    "from sklearn import metrics\n",
    "\n",
    "accuracy=metrics.r2_score(np.expm1(train_labels),np.expm1(blended_predictions(train)))\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the predictions for each model\n",
    "sns.set_style(\"white\")\n",
    "fig = plt.figure(figsize=(24, 12))\n",
    "\n",
    "ax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\n",
    "for i, score in enumerate(scores.values()):\n",
    "    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n",
    "\n",
    "plt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\n",
    "plt.xlabel('Model', size=20, labelpad=12.5)\n",
    "plt.tick_params(axis='x', labelsize=13.5)\n",
    "plt.tick_params(axis='y', labelsize=12.5)\n",
    "\n",
    "plt.title('Scores of Models', size=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_prices = blended_predictions(test)\n",
    "print(predicted_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.floor(np.expm1(blended_predictions(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = np.expm1(blended_predictions(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK = pd.DataFrame({'price': submission})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "MK.to_csv('김민구_3차.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
